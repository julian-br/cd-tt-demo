---
import "../styles/global.css";
import AudioRow from "../components/AudioRow.astro";
import AudioPlayer from "../components/AudioPlayer.astro";
import Experiment from "../components/Experiment.astro";

import pianoGT from "../assets/audio/gt/piano_gt.wav";
import guitarGT from "../assets/audio/gt/guitar_gt.wav";
import saxophoneGT from "../assets/audio/gt/saxophone_gt.wav";

const gtSpecFolder = `${import.meta.env.BASE_URL}assets/spectrograms/gt`;
const pianoGTSpec = `${gtSpecFolder}/piano_gt.png`;
const guitarGTSpec = `${gtSpecFolder}/guitar_gt.png`;
const saxophoneGTSpec = `${gtSpecFolder}/saxophone_gt.png`;
---

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width" />
    <title>ðŸŽ¶ CD-TT Demo</title>
  </head>

  <body class="bg-slate-100 text-slate-800">
    <main class="max-w-5xl mx-auto p-4 sm:p-8">
      <h1 class="text-[2.05rem] font-bold mb-8">
        Cross Domain Timbre Transfer Using Deep Generative Models
      </h1>
      <p class="text-slate-500 text-sm mb-5 text-justify">
        These audio samples accompany the Master's Thesis, "Cross-Domain Timbre
        Transfer Using Deep Generative Models," exploring timbre transfer from
        the human voice to instruments. Operating at 44.1 kHz, the system is
        trained with unpaired data of voice and instrument recordings. We
        convert hummed audio into a Log-Mel spectrogram, which a Variational
        Autoencoder trained with adversarial loss (VAE-GAN) translates to the
        target instrument's timbre, before the neural vocoder BigVGAN generates
        the final waveform.
      </p>
      <div class="rounded-lg overflow-hidden shadow-sm max-w-xl mb-6 ml-1">
        <img
          src=`${import.meta.env.BASE_URL}assets/pipeline.png`
          alt="overview over the pipeline"
        />
      </div>
      <p class="text-slate-500 text-sm mb-10 text-justify">
        Data for voice audio is from
        <a
          href="https://huggingface.co/datasets/dadinghh2/HumTrans"
          class="text-slate-600 hover:underline font-bold">HumTrans Dataset</a
        > , and for saxophone from the <a
          href="https://grfia.dlsi.ua.es/audio-to-score/"
          class="text-slate-600 hover:underline font-bold"
          >real_a2s_sax_dataset</a
        >.
      </p>

      <div class="mb-8">
        <h2 class="font-semibold text-md text-slate-700 mb-2">
          Real Instrument Samples
        </h2>
        <p class="text-slate-500 text-sm max-w-3xl mb-2 text-justify">
          For comparison, here are real audio samples for each of the three
          target instruments, taken from the training datasets.
        </p>
        <div class="flex gap-1 ml-1">
          <div class="rounded-lg px-2 pb-1 shadow-sm bg-white">
            <span class="text-xs pl-1 font-semibold text-slate-600"
              >E-Piano:</span
            >
            <AudioPlayer src={pianoGT} specPath={pianoGTSpec} />
          </div>
          <div class="rounded-lg px-2 pb-1 bg-white shadow-sm text-slate-600">
            <span class="text-xs font-semibold">Saxophone:</span>
            <AudioPlayer src={saxophoneGT} specPath={saxophoneGTSpec} />
          </div>
          <div class="rounded-lg px-2 pb-1 bg-white shadow-sm text-slate-600">
            <span class="text-xs font-semibold">Guitar:</span>
            <AudioPlayer src={guitarGT} specPath={guitarGTSpec} />
          </div>
        </div>
      </div>

      <div class="border-b border-slate-200">
        <nav id="tabs" class="-mb-px flex space-x-6" aria-label="Tabs">
          <button data-tab-target="#diff_instrum" class="tab active-tab"
            >Different Instruments</button
          >
          <button data-tab-target="#changes" class="tab"
            >Architectual Changes</button
          >
          <button data-tab-target="#sinus" class="tab"
            >Simplified Source Signal</button
          >
          <button data-tab-target="#data_param" class="tab"
            >Data and Param Size</button
          >
          <button data-tab-target="#bonus" class="tab">Bonus</button>
        </nav>
      </div>

      <div class="py-4">
        <div id="diff_instrum" class="tab-content">
          <Experiment
            title="Generated Instrument Samples"
            text="The following audio examples were generated from voices included in the training set but from melodies in the held-out test set. Samples are selected randomly and not cherry-picked."
          >
            <AudioRow title="Input Melody (Hummed)" folderName="hum_mixed" />
            <AudioRow
              title="Generated E-piano"
              folderName="instruments/hum_mixed_2_epiano_match_base"
            />
            <AudioRow
              title="Generated Saxophone"
              folderName="instruments/hum_mixed_2_saxophone"
            />
            <AudioRow
              title="Generated Guitar"
              folderName="instruments/hum_mixed_2_guitar_jam"
            />
          </Experiment>

          <Experiment
            title="Generalization to Unseen Voices"
            text="This section demonstrates the model's generalization capabilities. The input hums are from voices that were not part of the training set, testing the model's performance on unseen voices."
            ><AudioRow title="Input Melody (Hummed)" folderName="hum_unseen" />
            <AudioRow
              title="Generated E-piano"
              folderName="instruments/unseen_piano"
            />
            <AudioRow
              title="Generated Saxophone"
              folderName="instruments/unseen_saxophone"
            />
            <AudioRow
              title="Generated Guitar"
              folderName="instruments/unseen_guitar"
            />
          </Experiment>
        </div>
      </div>
      <div id="changes" class="tab-content hidden">
        <p class="text-slate-500 text-sm max-w-2xl mb-5 text-justify">
          As part of this research, we investigated a range of architectural
          variations. Below are samples from the different directions explored.
        </p>
        <Experiment
          title="More Downsampling"
          text="We experimented with adding more downsample steps."
        >
          <AudioRow title="Input Melody (Hummed)" folderName="hum_mixed" />
          <AudioRow
            title="2 downsample steps"
            folderName="instruments/hum_mixed_2_epiano_match_base"
          />
          <AudioRow
            title="3 downsample steps"
            folderName="downsample/hum_mixed_2_epiano_3_down_16"
          />
          <AudioRow
            title="4 downsample steps"
            folderName="downsample/hum_mixed_2_epiano_4_down_8"
          />
          <AudioRow
            title="5 downsample steps"
            folderName="downsample/hum_mixed_2_epiano_5_down_4"
          />
        </Experiment>
        <Experiment
          title="Discriminator Architecture Comparison"
          text="We compared a PatchGAN discriminator against a single-output and a multi-scale architecture."
        >
          <AudioRow title="Input Melody (Hummed)" folderName="hum_mixed" />
          <AudioRow
            title="PatchGan Discriminator"
            folderName="instruments/hum_mixed_2_epiano_match_base"
          />
          <AudioRow
            title="Single Output Discriminator"
            folderName="discriminator/hum_mixed_2_epiano_match_sd"
          />
          <AudioRow
            title="Multi Scale Discriminator"
            folderName="discriminator/hum_mixed_2_epiano_match_msd"
          />
        </Experiment>
        <!-- <Experiment title="Cycle Loss">
          <AudioRow title="Input Melody (hummed)" folderName="hum_mixed" />
          <AudioRow
            title="With Cycle Loss"
            folderName="instruments/hum_mixed_2_epiano_match_base"
          />
          <AudioRow
            title="Without Cycle Loss"
            folderName="cycle/hum_mixed_2_epiano_match_no_cycle"
          />
          <AudioRow
            title="With SSIM for Cycle Loss"
            folderName="hum_mixed_2_epiano_match_cycle_ssim"
          />
        </Experiment> -->
        <Experiment
          title="Decaying the Reconstruction Losses"
          text="We experimented with decaying the reconstruction and cycle loss in later stages of training, giving the adverserial loss higher impact in later epochs."
        >
          <AudioRow title="Input Melody (Hummed)" folderName="hum_mixed" />
          <AudioRow
            title="Without Decay"
            folderName="instruments/hum_mixed_2_epiano_match_base"
          />
          <AudioRow
            title="Decay starting at Epoch 60"
            folderName="decay/hum_mixed_2_epiano_match_decay_recon_2"
          />
          <AudioRow
            title="Decay starting at Epoch 1"
            folderName="decay/hum_mixed_2_epiano_match_decay_recon_5"
          />
        </Experiment>
      </div>
      <div id="sinus" class="tab-content hidden">
        <p class="text-slate-500 text-sm max-w-2xl mb-5 text-justify">
          To test the impact of input complexity, we trained a model on a
          simplified representation: a pure sine wave resynthesized from the F0
          and RMS of the original hums and a second model on the sine wave
          enriched with harmonic overtones.
        </p>
        <div class="rounded-lg overflow-hidden shadow-xs max-w-md ml-1 mb-2">
          <img
            src=`${import.meta.env.BASE_URL}assets/sinus.png`
            alt="extracted sinus and sinus with overtones"
          />
        </div>
        <p class="text-slate-500 text-xs max-w-2xl mb-5 text-justify">
          <strong>a)</strong> Mel-spectrogram of a hummed melody, <strong
            >b)</strong
          > The sine wave synthesized from the F0 and RMS of the original hummed
          melody <strong>c)</strong>
          synthesized sine wave with added harmonic overtones (sawtooth-like)
        </p>
        <Experiment
          title="Sine Wave"
          text="This experiment uses a pure sine wave, synthesized from the hum's extracted fundamental frequency (F0), as the training input."
        >
          <AudioRow title="Input Melody (Hummed)" folderName="hum_mixed" />

          <AudioRow title="Synthesized Sine Wave" folderName="sinus/sinus" />
          <AudioRow
            title="Trained on Sine"
            folderName="sinus/hum_mixed_sinus_2_epiano_match"
          />
          <AudioRow
            title="Trained on Hum"
            folderName="instruments/hum_mixed_2_epiano_match_base"
          />
        </Experiment>
        <Experiment
          title="Sine Wave + Overtones (sawtooth-like)"
          text="Based on the previous result, this experiment tests a richer signal. The model was trained on a sine wave enriched with harmonic overtones to provide more spectral information."
        >
          <AudioRow title="Input Melody (Hummed)" folderName="hum_mixed" />

          <AudioRow
            title="Synthesized Sine + Overtones"
            folderName="sinus/sinus_ot"
          />
          <AudioRow
            title="Trained on Sine + Overtones"
            folderName="sinus/hum_mixed_sinus_ot_2_epiano_match"
          />
          <AudioRow
            title="Trained on Hum"
            folderName="instruments/hum_mixed_2_epiano_match_base"
          />
        </Experiment>
      </div>
      <div id="data_param" class="tab-content hidden">
        <Experiment
          title="Impact of Training Data Size"
          text="We analyzed the model's data efficiency by training it on progressively smaller subsets, ranging from the full 120-minute baseline down to a minimum of only 12 minutes of audio."
        >
          <AudioRow title="Input Melody (Hummed)" folderName="hum_mixed" />
          <AudioRow
            title="100 % (120 min)"
            folderName="instruments/hum_mixed_2_epiano_match_base"
          />
          <AudioRow
            title="75% (90 min)"
            folderName="data_size/hum_mixed_2_epiano_match_data_75"
          />
          <AudioRow
            title="50% (60 min)"
            folderName="data_size/hum_mixed_2_epiano_match_data_50"
          />
          <AudioRow
            title="25% (30 min)"
            folderName="data_size/hum_mixed_2_epiano_match_data_25"
          />
          <AudioRow
            title="10% (12 min)"
            folderName="data_size/hum_mixed_2_epiano_match_data_10"
          />
        </Experiment>
        <Experiment
          title="Impact of Model Size"
          text="To investigate the trade-off between model complexity and output quality we trained several smaller versions of the model, with parameter reductions focused on the encoder and generator architecture (components used during inference)."
        >
          <AudioRow title="Input Melody (hummed)" folderName="hum_mixed" />
          <AudioRow
            title="Baseline (2.69 M)"
            folderName="instruments/hum_mixed_2_epiano_match_base"
          />
          <AudioRow
            title="L (2.10 M)"
            folderName="param_size/hum_mixed_2_epiano_match_size_L"
          />
          <AudioRow
            title="M (0.85 M)"
            folderName="param_size/hum_mixed_2_epiano_match_size_M"
          />
          <AudioRow
            title="S (0.23 M)"
            folderName="param_size/hum_mixed_2_epiano_match_size_S"
          />
        </Experiment>
      </div>
      <div id="bonus" class="tab-content hidden">
        <h2 class="text-2xl font-semibold text-slate-700">Bonus</h2>
        <p class="text-slate-500 text-sm mb-10 text-justify">
          Just out of curiosity, we fed the models trained on hummed voice with
          audio from completely different audio sources. The samples are taken
          from the <a
            href="https://philharmonia.co.uk/resources/sound-samples/"
            class="text-blue-400 hover:underline font-bold"
            >Philharmonia Orchestra" Sound Library</a
          > (single instrument), the <a
            href="https://www.openslr.org/12"
            class="text-blue-400 hover:underline font-bold"
            >LibriSpeech ASR corpus</a
          > (speech), and the <a
            href="https://github.com/karolpiczak/ESC-50"
            class="text-blue-400 hover:underline font-bold">ESC-50 dataset</a
          > (sound effects).
        </p>
        <Experiment title="Intruments" text="">
          <AudioRow title="Input" folderName="bonus/input/inst" />
          <AudioRow title="Generated E-Piano" folderName="bonus/piano/inst" />
          <AudioRow
            title="Generated Saxophone"
            folderName="bonus/saxophone/inst"
          />
          <AudioRow title="Generated Guitar" folderName="bonus/guitar/inst" />
        </Experiment>
        <Experiment title="Speech" text="">
          <AudioRow title="Input" folderName="bonus/input/speech" />
          <AudioRow title="Generated E-Piano" folderName="bonus/piano/speech" />
          <AudioRow
            title="Generated Saxophone"
            folderName="bonus/saxophone/speech"
          />
          <AudioRow title="Generated Guitar" folderName="bonus/guitar/speech" />
        </Experiment>
        <Experiment title="Sound Effects" text="">
          <AudioRow title="Input" folderName="bonus/input/env" />
          <AudioRow title="Generated E-Piano" folderName="bonus/piano/env" />
          <AudioRow
            title="Generated Saxophone"
            folderName="bonus/saxophone/env"
          />
          <AudioRow title="Generated Guitar" folderName="bonus/guitar/env" />
        </Experiment>
      </div>
    </main>

    <footer class="text-center mt-12 text-slate-400">
      <p>&copy; {new Date().getFullYear()} Julian Berner</p>
    </footer>
  </body>

  <script>
    const tabsContainer = document.getElementById("tabs");
    if (tabsContainer) {
      const tabContents = document.querySelectorAll(".tab-content");
      const tabs = tabsContainer.querySelectorAll(".tab");

      tabsContainer.addEventListener("click", (event) => {
        const clickedTab = (event.target as Element).closest<HTMLButtonElement>(
          ".tab"
        );
        if (!clickedTab) return;

        const targetSelector = clickedTab.dataset.tabTarget;

        if (!targetSelector) return;

        tabs.forEach((tab) => tab.classList.remove("active-tab"));
        tabContents.forEach((content) => content.classList.add("hidden"));

        clickedTab.classList.add("active-tab");

        const targetContent = document.querySelector(targetSelector);
        if (targetContent) {
          targetContent.classList.remove("hidden");
        }
      });
    }
  </script>
</html>
